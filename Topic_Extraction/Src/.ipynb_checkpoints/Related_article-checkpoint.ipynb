{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from urllib.request import Request\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "import pyLDAvis.gensim\n",
    "from gensim.models import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "from nltk.tokenize import word_tokenize\n",
    "from openpyxl import load_workbook\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#declare parameters\n",
    "LinkNYC_Coverage_Tracker = os.environ['LINKNYC_CONVERAGE_TRACKER']\n",
    "LDAresults =  os.environ['LDA_RESULTS']\n",
    "\n",
    "#custom stopwords\n",
    "not_want = ['de','xexx','xcxa','u','le','la','en','xexxbnback','also','said','york']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create functions\n",
    "def scrapper(i,output):\n",
    "    '''\n",
    "    perform scrapping of hyperlinks from articles related to intersection co\n",
    "    '''\n",
    "    corpus = list()\n",
    "    for i in range(len(hyperlink[0])):\n",
    "    try:\n",
    "        req = Request(hyperlink[0][i], headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urllib.request.urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage)\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract() \n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        # break multi-headlines into a line each\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        # drop blank lines\n",
    "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "        corpus.append(text)\n",
    "    except:\n",
    "        pass\n",
    "    corpus_all = [x.encode('utf-8') for x in corpus]\n",
    "    with open(output, \"w\") as output:\n",
    "        output.write(str(corpus_all))\n",
    "        \n",
    "def prepocess(text):\n",
    "    '''\n",
    "    keep alphanumeric characters only\n",
    "    lower all characters\n",
    "    remove special characters\n",
    "    remove leading or trailing spaces\n",
    "    '''\n",
    "    return re.sub(r\"[^a-zA-Z0-9]+\", ' ', re.sub(r'\\d+', '', text.lower()).translate(str.maketrans('', '', string.punctuation)).strip())\n",
    "\n",
    "def top_model(corpus):\n",
    "    '''\n",
    "    perfom LDA modeling\n",
    "    removes english stop words\n",
    "    perfoms tokenization\n",
    "    lemmanizes words\n",
    "    uses bag of words\n",
    "    '''\n",
    "    corpus_topic = []\n",
    "    for sent in corpus:\n",
    "        corpus_topic.append(prepocess(corpus[0]))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    tokens = word_tokenize(str(corpus_topic))\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    result = [i for i in result if not i in not_want]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    preprocess_text = []\n",
    "    for word in result:\n",
    "        preprocess_text.append(lemmatizer.lemmatize(word))\n",
    "    dictionary = gensim.corpora.Dictionary([d.split() for d in preprocess_text])\n",
    "    bow_corpus = [dictionary.doc2bow(doc) for doc in[d.split() for d in preprocess_text]]\n",
    "    lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 6, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 10,\n",
    "                                   workers = 2)\n",
    "    topics = lda_model.print_topics(num_words=5)\n",
    "    for topic in topics:\n",
    "        print(topic)\n",
    "\n",
    "def viz_model(text,outputloc):\n",
    "    '''\n",
    "    generate word cloud\n",
    "    '''\n",
    "    corpus_topic = []\n",
    "    for sent in text:\n",
    "        corpus_topic.append(prepocess(text[0]))\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = word_tokenize(str(corpus_topic))\n",
    "    result = [i for i in tokens if not i in stop_words]\n",
    "    result = [i for i in result if not i in not_want]\n",
    "    lemmatizer=WordNetLemmatizer()\n",
    "    preprocess_text = []\n",
    "    for word in result:\n",
    "        preprocess_text.append(lemmatizer.lemmatize(word))\n",
    "    dictionary = gensim.corpora.Dictionary([d.split() for d in preprocess_text])\n",
    "    corpus = [dictionary.doc2bow(doc) for doc in[d.split() for d in preprocess_text]]\n",
    "    ldamodel = LdaModel(corpus=corpus, num_topics=6, id2word=dictionary)\n",
    "    pyLDAvis.enable_notebook()\n",
    "    pyLDAvis.save_html(pyLDAvis.gensim.prepare(ldamodel, corpus, dictionary), outputloc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_dict = []\n",
    "sheetName = ['2018','2017','2016','2015','Greatest Hits']\n",
    "for sheetn in sheetName:\n",
    "    ws_dict.append(pd.read_excel(LinkNYC_Coverage_Tracker, sheetname=sheetn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink = []\n",
    "for i in range(len(ws_dict)):\n",
    "    hyperlink.append(ws_dict[i].URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperlink_index = [0,1,2,3,4]\n",
    "output_files = [\"text_2018.txt\",\"text_2017.txt\",\"text_2015.txt\",\"Greatest_Hits.txt\"]\n",
    "\n",
    "#create output\n",
    "for i in range(len(output_files)):\n",
    "    scrapper(hyperlink_index,output_files[i])\n",
    "    #create python variable\n",
    "    with open(output_files[i], 'r') as f:\n",
    "        exec(\"{}\".format(output_files[i].replace(\".txt\",\"\"))+\" = f.readlines()\")\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.058*\"wifi\" + 0.049*\"new\" + 0.022*\"public\" + 0.020*\"day\" + 0.020*\"booth\"')\n",
      "(1, '0.011*\"business\" + 0.010*\"good\" + 0.008*\"would\" + 0.007*\"find\" + 0.007*\"home\"')\n",
      "(2, '0.015*\"get\" + 0.011*\"news\" + 0.010*\"company\" + 0.009*\"digital\" + 0.009*\"gigabit\"')\n",
      "(3, '0.011*\"finally\" + 0.010*\"million\" + 0.009*\"hotspot\" + 0.009*\"say\" + 0.009*\"right\"')\n",
      "(4, '0.026*\"free\" + 0.019*\"first\" + 0.015*\"access\" + 0.012*\"service\" + 0.011*\"next\"')\n",
      "(5, '0.034*\"city\" + 0.024*\"phone\" + 0.023*\"hub\" + 0.020*\"kiosk\" + 0.017*\"woman\"')\n"
     ]
    }
   ],
   "source": [
    "#generate 2015 LDA results\n",
    "top_model(text_2015)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.095*\"new\" + 0.024*\"wifi\" + 0.020*\"free\" + 0.012*\"call\" + 0.011*\"technology\"')\n",
      "(1, '0.011*\"street\" + 0.011*\"use\" + 0.010*\"network\" + 0.009*\"user\" + 0.008*\"one\"')\n",
      "(2, '0.058*\"city\" + 0.013*\"first\" + 0.013*\"digital\" + 0.011*\"public\" + 0.010*\"news\"')\n",
      "(3, '0.030*\"linknyc\" + 0.018*\"phone\" + 0.017*\"display\" + 0.016*\"people\" + 0.013*\"year\"')\n",
      "(4, '0.066*\"kiosk\" + 0.024*\"time\" + 0.018*\"make\" + 0.018*\"company\" + 0.014*\"service\"')\n",
      "(5, '0.016*\"information\" + 0.015*\"main\" + 0.014*\"link\" + 0.010*\"business\" + 0.010*\"photo\"')\n"
     ]
    }
   ],
   "source": [
    "#generate greatest_hit LDA results\n",
    "top_model(Greatest_Hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.025*\"pm\" + 0.015*\"time\" + 0.015*\"nyc\" + 0.014*\"june\" + 0.013*\"news\"')\n",
      "(1, '0.016*\"free\" + 0.015*\"ny\" + 0.014*\"make\" + 0.011*\"use\" + 0.011*\"toronto\"')\n",
      "(2, '0.015*\"data\" + 0.014*\"park\" + 0.013*\"get\" + 0.013*\"work\" + 0.012*\"business\"')\n",
      "(3, '0.081*\"city\" + 0.079*\"new\" + 0.018*\"linknyc\" + 0.015*\"street\" + 0.010*\"day\"')\n",
      "(4, '0.023*\"kiosk\" + 0.017*\"year\" + 0.016*\"smart\" + 0.015*\"company\" + 0.015*\"technology\"')\n",
      "(5, '0.020*\"one\" + 0.018*\"service\" + 0.017*\"public\" + 0.017*\"u\" + 0.015*\"project\"')\n"
     ]
    }
   ],
   "source": [
    "#generate 2017 LDA results\n",
    "top_model(text_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, '0.021*\"data\" + 0.018*\"free\" + 0.018*\"linknyc\" + 0.014*\"medium\" + 0.013*\"puerto\"')\n",
      "(1, '0.079*\"city\" + 0.013*\"business\" + 0.011*\"privacy\" + 0.011*\"would\" + 0.011*\"need\"')\n",
      "(2, '0.022*\"service\" + 0.019*\"smart\" + 0.015*\"phone\" + 0.010*\"way\" + 0.010*\"june\"')\n",
      "(3, '0.044*\"kiosk\" + 0.026*\"wifi\" + 0.026*\"public\" + 0.022*\"people\" + 0.017*\"one\"')\n",
      "(4, '0.075*\"new\" + 0.022*\"digital\" + 0.020*\"technology\" + 0.019*\"company\" + 0.018*\"u\"')\n",
      "(5, '0.015*\"news\" + 0.014*\"information\" + 0.010*\"policy\" + 0.009*\"community\" + 0.009*\"work\"')\n"
     ]
    }
   ],
   "source": [
    "#generate 2018 LDA results\n",
    "top_model(text_2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in output_files:\n",
    "    viz_model(i,LDAresults+i.replace('.txt',\".html\").replace(\"text\",\"lda\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
